{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4804498b",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è<span style=\"color:#88dad3;\"><strong>LLM Workshop 1</span>: Building a Local Chatbot using Streamlit + Langchain + Ollama\n",
    "\n",
    "This walkthrough will help you understand and build a simple **Local Chatbot App** using:\n",
    "\n",
    "- üêç **Python**\n",
    "- üíª **Streamlit** ‚Äî for the UI\n",
    "- ü¶ô **LangChain + Ollama** ‚Äî to run LLMs locally\n",
    "- üí¨ **LLaMA3.2** (or any local model via Ollama)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced3538b",
   "metadata": {},
   "source": [
    "## üß© Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922cc60d",
   "metadata": {},
   "source": [
    "### üñºÔ∏è <span style=\"color:#88dad3;\"> UI Initialization </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a0610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.title(\"My Local Chatbot ü§≠\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fd6770",
   "metadata": {},
   "source": [
    "### üíæ <span style=\"color:#88dad3;\"> Initializing Chat History </span>\n",
    "\n",
    "We use `st.session_state` to store the conversation history between the user and the assistant.\n",
    "\n",
    "- `session_state` is like memory for the app.\n",
    "- We check if `\"messages\"` exists, and if not, we create an empty list.\n",
    "- This makes sure our chatbot remembers past messages as we interact with it.\n",
    "\n",
    "> üí° Without this, the chat would reset every time we type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12d64a",
   "metadata": {},
   "source": [
    "### üí¨ Displaying Chat Messages\n",
    "\n",
    "We loop through all messages stored in `st.session_state.messages` and display them using:\n",
    "\n",
    "- `st.chat_message(\"user\")` or `st.chat_message(\"assistant\")` ‚Äî shows the message in the correct \"bubble\"\n",
    "- `st.markdown(message[\"content\"])` ‚Äî displays the text in a markdown-friendly way\n",
    "\n",
    "Once submitted, we append the user's input to the messages history.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5040d",
   "metadata": {},
   "source": [
    "### üôã‚Äç‚ôÄÔ∏è Receiving User Input\n",
    "\n",
    "This creates a chat box where the user can type their message.\n",
    "\n",
    "- `st.chat_input(\"Write something\")` lets the user type a message.\n",
    "- If the user enters something, it's saved into the chat history.\n",
    "\n",
    "üîç Notes\n",
    "\n",
    "- The dictionary `{\"role\": \"user\", \"content\": prompt}` keeps track of **who** sent the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prompt := st.chat_input(\"Write something\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe802b30",
   "metadata": {},
   "source": [
    "### üë§ Displaying User's Message \n",
    "\n",
    "\n",
    "This shows the user‚Äôs latest message in a chat bubble right after they send it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with st.chat_message(\"user\"):\n",
    "    st.markdown(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528e14b",
   "metadata": {},
   "source": [
    "### üß† Initialize the Assistant's LLM with Ollama\n",
    "\n",
    "- We import `ChatOllama` from `langchain_ollama` to connect with our local LLM.\n",
    "- Inside the assistant‚Äôs chat message block, we:\n",
    "  - Select the local model (`llama3.2`).\n",
    "  - Create an instance of the language model with a temperature of 0.7 to control response creativity.\n",
    "  \n",
    "Langchain‚Äôs Ollama integration lets us run local LLMs like llama3.2.\n",
    "\n",
    "temperature=0.7 makes the output more creative while retaining coherence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb176f4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"font-size: 1.1em; font-weight: 600; color:rgb(23, 114, 119); cursor: pointer;\">\n",
    "  ‚ÑπÔ∏è <span>What is langchain_ollama and ChatOllama </span>\n",
    "</summary>\n",
    "\n",
    "`langchain_ollama` is a LangChain integration that lets you easily connect to local LLMs powered by Ollama.\n",
    "\n",
    "- `ChatOllama` is the class that wraps your local model for chat-based interactions.\n",
    "- It handles sending messages, streaming responses, and configuring parameters like temperature.\n",
    "\n",
    "This makes it simple to plug your local LLM into a chatbot or other LangChain-powered applications.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 1.1em; font-weight: 600; color:rgb(24, 129, 156); cursor: pointer;\">\n",
    "  üå°Ô∏è <span >Understanding Temperature in Language Models</span>\n",
    "</summary>\n",
    "\n",
    "The **temperature** parameter controls how random or creative the model's responses are:\n",
    "\n",
    "- <span style=\"color:green; font-weight:bold;\">Low temperature (0.0‚Äì0.3):</span><br>  \n",
    "  More deterministic, precise, and predictable.<br>  \n",
    "  Great for factual or technical tasks.\n",
    "\n",
    "- <span style=\"color:orange; font-weight:bold;\">Medium temperature (0.5‚Äì0.7):</span><br>  \n",
    "  Balanced creativity and accuracy.<br>  \n",
    "  Ideal for general chatting, summarizing, and coding.\n",
    "\n",
    "- <span style=\"color:red; font-weight:bold;\">High temperature (0.8‚Äì1.0+):</span><br>  \n",
    "  More creative and unpredictable.<br>  \n",
    "  Perfect for brainstorming and storytelling.\n",
    "\n",
    "Adjusting temperature helps you control how ‚Äúsafe‚Äù or ‚Äúwild‚Äù the generated answers feel.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858de903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "with st.chat_message(\"assistant\"):\n",
    "    local_model = \"llama3.2\"\n",
    "    llm = ChatOllama(model=local_model, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41792555",
   "metadata": {},
   "source": [
    "### üì§ Stream the Assistant's Response\n",
    "\n",
    "\n",
    "- We send the entire chat history (`st.session_state.messages`) to the language model as input.\n",
    "- The `.stream()` method from ChatOllama generates the assistant‚Äôs reply **incrementally**, simulating real-time typing.\n",
    "- `st.write_stream(stream)` displays the response as it arrives, creating a smooth chat experience.\n",
    "\n",
    "> üí° Streaming makes the bot feel more interactive and responsive compared to waiting for the full answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a022701",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = llm.stream(\n",
    "    input=[\n",
    "        {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n",
    "        for m in st.session_state.messages\n",
    "    ]\n",
    ")\n",
    "response = st.write_stream(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d8223",
   "metadata": {},
   "source": [
    "### üóÉÔ∏è Saving the Assistant‚Äôs Response\n",
    "\n",
    "- After receiving the assistant‚Äôs reply, we add it to the chat history.\n",
    "- This keeps the conversation **persistent** so the full chat shows in future interactions.\n",
    "- Storing messages in `st.session_state.messages` ensures the chat updates correctly with every new message.\n",
    "\n",
    "> ‚úÖ This step is essential to maintain the conversation flow and context throughout the chat session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8774e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae40139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.0.0.2:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! streamlit run ./03_chatbot.py\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "üí° My Lovely LLM Workshop 1",
   "language": "python",
   "name": "llm_w1_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
