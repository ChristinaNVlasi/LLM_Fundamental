{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a67c517",
   "metadata": {},
   "source": [
    "# üß† <span style=\"color:#88dad3;\"><strong>LLM Workshop 1</span>: Local Setup with Ollama</strong>\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Let‚Äôs Begin with the Setup\n",
    "\n",
    "To start working with cutting-edge **Large Language Models (LLMs)** on your **local machine**, we‚Äôll use an open-source platform for running large language models (LLMs) locally called [**Ollama**](https://ollama.com).\n",
    "\n",
    "You‚Äôll be able to run models like `llama3.2` without relying on cloud services or APIs ‚Äî everything happens **locally and securely**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b3d68d",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è <span style=\"color:#f67b65;\">Step 1:</span> Install Ollama\n",
    "\n",
    "### üõ†Ô∏è What is Ollama?\n",
    "\n",
    "> **Ollama** is a locally deployed AI model runner for open-source LLMs ‚Äî it makes running models as easy as typing one command in your terminal.\n",
    "\n",
    "No setup headaches. No cloud dependency. Just models, running directly on your computer.\n",
    "\n",
    "\n",
    "\n",
    "### üì• Download & Install\n",
    "\n",
    "- ‚úÖ Platforms supported: `macOS`, `Windows`, `Linux`  \n",
    "- üîó Download here: [**ollama.com/download**](https://ollama.com/download)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d067e",
   "metadata": {},
   "source": [
    "### After installing Ollama, open a terminal window and run these commands to pull and start the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama pull llama3.2\n",
    "\n",
    "# ollama run llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed64acf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d3ff45",
   "metadata": {},
   "source": [
    "# <span style=\"color:#f67b65;\">üêç Step 2:</span>  Setting Up a Python Environment with `uv`\n",
    "\n",
    "Now, open a new terminal window. We‚Äôll use [`uv`](https://github.com/astral-sh/uv), a fast, modern alternative to `pip` and `venv`, to create and manage a Python environment for this workshop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f1b8d",
   "metadata": {},
   "source": [
    "### 1. üì• Install `uv` (if not already installed)\n",
    "\n",
    "```bash\n",
    "pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install uv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ad0f2",
   "metadata": {},
   "source": [
    "### 2. üìÅ Navigate to your project folder\n",
    "\n",
    "```bash\n",
    "cd path/to/your/project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f562ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Example:\n",
    "\n",
    "# cd ./GitHub/LLM_Fundamental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd9fe73",
   "metadata": {},
   "source": [
    "### 3. üõ†Ô∏è Create a virtual environment\n",
    "\n",
    "```bash\n",
    "uv venv LLM_w1 --python=python3.11\n",
    "```\n",
    "- LLM_w1 ‚Üí name of the environment folder\n",
    "\n",
    "- --python=python3.11 ‚Üí optional: use a specific Python version\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc10dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv venv LLM_env1 --python=python3.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea46aae",
   "metadata": {},
   "source": [
    "### 4. üß™ Activate the environment\n",
    "\n",
    "```bash\n",
    "source LLM_w1/bin/activate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source LLM_env1/bin/activate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7253868",
   "metadata": {},
   "source": [
    "### 5. üëâ Install required packages using uv pip:\n",
    "\n",
    "```bash\n",
    "uv pip install \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv pip install litellm streamlit langchain langchain-ollama langchain_community pymupdf langchain-text-splitters faiss-cpu docling ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221ceb9",
   "metadata": {},
   "source": [
    "### 6. ‚ñ∂Ô∏è Register the environment as a Jupyter kernel\n",
    "\n",
    "```bash\n",
    "\n",
    "python -m ipykernel install --user --name=LLM_w1_1 --display-name \"üí° My Lovely LLM Workshop 1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f9a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m ipykernel install --user --name=LLM_env1 --display-name \"üí° My Lovely LLM Workshop 1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6ef5a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e289e",
   "metadata": {},
   "source": [
    "# üì° <span style=\"color:#f67b65;\"> Step 3:</span> Verify Ollama Model Setup with LiteLLM \n",
    "\n",
    "Now that Ollama is installed and your environment is configured, it‚Äôs time to **verify that everything is working correctly**.  \n",
    "\n",
    "In this step, you will perform a simple test using **LiteLLM** to communicate with the Ollama model running locally. LiteLLM is an open-source Python library that provides a unified interface for interacting with a wide range of LLM APIs. It simplifies the process of integrating with various LLM providers by allowing developers to use the same input/output format regardless of the underlying provider. \n",
    "\n",
    "\n",
    "\n",
    "### üéØ Goals for this step:\n",
    "\n",
    "- ‚úÖ **Ollama server** is running and reachable at `http://localhost:11434`  \n",
    "- üß© **LiteLLM** can successfully send requests to the model  \n",
    "- üì¨ You receive meaningful and correct **responses** from the model  \n",
    "\n",
    "<details>\n",
    "<summary>üîß How It Works</summary>\n",
    "\n",
    "This code sends a message to the Ollama model (`llama3.2`) running on your local server.\n",
    "\n",
    "The **completion** function constructs the prompt, sends it to the Ollama model via LiteLLM, waits for the model to generate a response, and then returns that response.\n",
    "\n",
    "It asks the model a question and waits for its reply.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üì¶ What Does It Return</summary>\n",
    "\n",
    "The function returns a response containing the model's answer.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° Why Use This Code</summary>\n",
    "\n",
    "This code tests that your Ollama model and LiteLLM setup are working.\n",
    "\n",
    "It shows how to send prompts and get answers from the model via the API.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "### üîÑ Flow\n",
    "\n",
    "User üßë‚Äçüíª <br>\n",
    "‚Üì <br>\n",
    "LiteLLM üöÖ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚Üê Python wrapper to easily call local or remote LLMs.<br>\n",
    "‚Üì <br>\n",
    "Ollama ü¶ô &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚Üê runs the AI model locally<br>\n",
    "‚Üì <br>\n",
    "LLaMA 3 üß† &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚Üê generates the response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afcd2553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence designed to assist and communicate with users, providing helpful information and answering questions accurately always.\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama/llama3.2\",  \n",
    "    messages=[{ \"content\": \"respond in 20 words. who are you?\",\"role\": \"user\"}], \n",
    "    api_base=\"http://localhost:11434\"\n",
    ")\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "üí° My Lovely LLM Workshop 1",
   "language": "python",
   "name": "llm_w1_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
